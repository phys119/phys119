{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0607874-fa9a-44d1-a7af-2c5db485891a",
   "metadata": {},
   "source": [
    "# Prelab 08: Analytic chi-squared minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570b70d5-9afc-45b0-bd6a-56ba39311b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import data_entry2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c7a22-ebd3-4977-9d87-395137731c65",
   "metadata": {},
   "source": [
    "In Lab 08 you will continue to collect data from the RC circuit in an effort to improve your dataset. Additionally, this prelab will provide you with an analytic formula to help you automatically **calculate** the fit parameters that correspond to the best possible fit of your model to your data.\n",
    "\n",
    "**An important reminder: the \"best fit\" is not the same as a \"good fit\".** Even though the analytic formula will give you the best fit of your model to your data, it does not allow you to say \"this is a *good* fit\". The best fit, by definition, is one that minimizes chi-squared, but a minimized chi-squared of $\\chi^2=11.8$ or one with a strong pattern in the residuals is still not considered a good fit to your data. Conversely, a minimized chi-squared of $\\chi^2=0.05$ still requires you to take a closer look at your uncertainty estimation strategy, since the uncertainties would likely have been overestimated. This automated method of chi-squared minimization still requires you to use residuals plots and your other techniques for assessing goodness of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d44b16-605b-4d2c-b5f6-a5679404bc55",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8.1 The analytic formula for minimizing chi-squared for a one-parameter model, $y = mx$, with no y-intercept\n",
    "_This section details deriving the analytic formula for minimizing chi-squared for the one-parameter model, $y = mx$, while the next section provides the solution to the two-parameter model, $y = mx + b$. The solution for the two-parameter model is the one we will be using in the lab, but it is much more complicated so we start with the one-parameter model as an intermediate step._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ee9a5-d5b5-47bf-b100-b396faa1bc8e",
   "metadata": {},
   "source": [
    "Recall that chi-squared is a continuous function of the fitting parameters in your model, meaning that if you change one of the fitting parameters (such as increasing or decreasing the slope), this will change the resulting value of chi-squared. If you have $P$ parameters in your model, then chi-squared is a $P$-dimensional function. For instance, if we are fitting a one-parameter linear model, $y=mx$, then $m$ is the sole parameter and the associated chi-squared function is $\\chi^2(m)$. For a two-parameter linear model, $y=mx+b$, then we would have $\\chi^2(m,b)$. \n",
    "\n",
    "For a simple model function like a straight line, we can use calculus to analytically find the parameter values that minimize chi-squared instead of manually adjusting the parameters like we have been doing so far. Below, we'll work through that calculation for the case of a straight-line fit with no intercept. We work through each step in this derivation to make the process clear.\n",
    "\n",
    "From your calculus courses, you know that you can take a continuous function $f(x)$ and find the minimum or maximum in that function (a critical point, $x_c$) by taking the derivative of $f(x)$ with respect to $x$, setting this derivative equal to zero, and then solving the resulting equation:\n",
    "\n",
    "$$\\left[\\frac{df(x)}{dx}\\right]_{x=x_c}=0.$$\n",
    "\n",
    "Since $\\chi^2$ is a continuous function, we can follow this process to come up with an expression that automatically calculates the critical point(s), to find the minimum. In other words, we can use calculus to derive an *analytic* expression for the best fit parameter(s).\n",
    "\n",
    "*NOTE! for those thinking: wait a minute, how do we know the critical point will be a minimum and not a maximum? If we think about fitting a model to data, we know that as we move the parameters to $+\\infty$ and $-\\infty$ the fit will become increasingly worse, meaning that chi-squared has no maxima. From this observation is follows that any critical point found must be a minimum.*\n",
    "\n",
    "In the simplest case of a one-parameter linear model, $y=mx$, we wish to minimize chi-squared with respect to $m$ to find the best-fit slope,\n",
    "\n",
    "$$ \\frac{d\\chi^2(m)}{dm} = 0.$$\n",
    "\n",
    "We can first substitute our general expression for chi-squared\n",
    "\n",
    "$$ \\frac{d}{dm}\\left[ \\frac{1}{N-P} \\sum_{i=1}^N \\left(\\frac{y_i - f(x_i)}{\\delta y_i}\\right)^2 \\right] = 0.$$\n",
    "\n",
    "Our model is $f(x_i) = mx_i$, which we can substitute into the above expression\n",
    "\n",
    "$$ \\frac{d}{dm}\\left[ \\frac{1}{N-P} \\sum_{i=1}^N \\left(\\frac{y_i - mx_i}{\\delta y_i}\\right)^2 \\right] = 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a0ea8-14ed-41f6-bc95-eb3aa7aee936",
   "metadata": {},
   "source": [
    "If we differentiate the above and solve for $m$, we will find the slope ($m$) that corresponds to the lowest possible chi-squared. Here, we skip ahead to the solution, but include the full derivation in an appendix at the end of this document.\n",
    "\n",
    "$$ m = \\frac{\\displaystyle \\sum_{i=1}^N  \\frac{x_i y_i}{(\\delta y_i)^2}}{\\displaystyle \\sum_{i=1}^N \\frac{x_i^2}{(\\delta y_i)^2}}.$$\n",
    "\n",
    "*Note: if you look at how this equation was written in Markdown, you can see that we are using display-style notation to make the summation symbols and fractions align more aesthetically.*\n",
    "\n",
    "Given our $x$ and $y$ data (including uncertainty in $y$), we are able to analytically solve for the best-fit slope using this equation! The uncertainty in this slope can be determined from the uncertainties in the data by uncertainty propagation. The result is:\n",
    "\n",
    "$$ \\delta m = \\sqrt{\\frac{1}{\\displaystyle \\sum_{i=1}^N \\dfrac{x_i^2}{(\\delta y_i)^2}}} .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e8a627-0ea4-46a4-85da-1ec6cde33ba8",
   "metadata": {},
   "source": [
    "Since the term $\\sum_{i=1}^N \\frac{x_i^2}{(\\delta y_i)^2}$ appears both in $m$ and $\\delta m$, it is convenient for notation and coding purposes to define a placeholder variable\n",
    "\n",
    "$$ Z = \\sum_{i=1}^N \\dfrac{x_i^2}{(\\delta y_i)^2},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23059221-5598-442b-aa3b-b9a5397e0a90",
   "metadata": {},
   "source": [
    "such that\n",
    "\n",
    "$$m = \\dfrac{1}{Z} \\sum_{i=1}^N  \\dfrac{x_iy_i}{(\\delta y_i)^2},$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\delta m = \\sqrt{\\dfrac{1}{Z}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d610e4-5217-4038-8147-555422212c01",
   "metadata": {},
   "source": [
    "**Your turn #1:** Take a close look at the expression for $\\delta m$. How do (A) the number of data points, $N$, and (B) the uncertainies in the data, $\\delta y_i$, impact $\\delta m$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad46f59-2db0-4447-acea-cd032c4b3553",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0e4954c-f95d-42bc-9731-965a015f465b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### **Uncollapse for answer to Your Turn #1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cce2d45-9200-4c46-82e9-e64e2aa5287b",
   "metadata": {},
   "source": [
    "a) $Z$ is a sum over each data point, without a 1/$N$ term, so each additional point will always increase $Z$. Since $Z$ is in the denominator, $\\delta m$ will get smaller for each additional data point added.\n",
    "\n",
    "b) Similarly, the smaller $\\delta y$, the larger $Z$, and thus the smaller $\\delta m$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec2bd40-54e0-4857-9849-5d585ad7b4f1",
   "metadata": {},
   "source": [
    "## 8.2 The analytic formulae for minimizing chi-squared for the two-parameter model, $y = mx + b$\n",
    "In this section we jump straight into the analytic formulas for for minimizing chi-squared for the two-parameter model, $y = mx + b$. The process is essentially the same as for the one-parameter model above, except we will now also find $b$ by differerntiating chi-squared with respect to $b$ and setting that to 0 to find the minimum. Our initial expressions are then as follows:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial m}\\left[ \\frac{1}{N-P} \\sum_{i=1}^N \\left(\\frac{y_i - (mx_i + b)}{\\delta y_i}\\right)^2 \\right] = 0,$$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial b}\\left[ \\frac{1}{N-P} \\sum_{i=1}^N \\left(\\frac{y_i - (mx_i + b)}{\\delta y_i}\\right)^2 \\right] = 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c284f1-c025-4dd0-936a-a87d51779ec2",
   "metadata": {},
   "source": [
    "After differentiation and some algebra, we find that the solutions for a weighted fit to the two-parameter model $y=mx + b$ are\n",
    "\n",
    "$$m = \\frac{1}{Z} \\left(\n",
    "\\sum_{i=1}^N \\frac{1}{(\\delta y_i)^2} \\cdot \\sum_{i=1}^N \\frac{x_i y_i}{(\\delta y_i)^2} \n",
    "- \\sum_{i=1}^N \\frac{x_i}{(\\delta y_i)^2} \\cdot \\sum_{i=1}^N \\frac{y_i}{(\\delta y_i)^2}\n",
    "\\right),$$\n",
    "\n",
    "and\n",
    "\n",
    "$$b = \\frac{1}{Z} \\left(\n",
    "\\sum_{i=1}^N \\frac{x_i^2}{(\\delta y_i)^2} \\cdot \\sum_{i=1}^N \\frac{y_i}{(\\delta y_i)^2} \n",
    "- \\sum_{i=1}^N \\frac{x_i}{(\\delta y_i)^2} \\cdot \\sum_{i=1}^N \\frac{x_i y_i}{(\\delta y_i)^2}\n",
    "\\right),$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f6a335-649d-4216-9c79-aac7494688ca",
   "metadata": {},
   "source": [
    "where $Z$ is a placeholder variable (different from the 1-parameter version) defined as\n",
    "\n",
    "$$ Z = \\sum_{i=1}^N \\frac{1}{(\\delta y_i)^2} \\cdot \\sum_{i=1}^N \\frac{x_i^2}{(\\delta y_i)^2}\n",
    "- \\left( \\sum_{i=1}^N \\frac{x_i}{(\\delta y_i)^2} \\right)^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c94761-3caa-4bc3-be14-e0300d3450c4",
   "metadata": {},
   "source": [
    "The uncertainty in the fit parameters are given by\n",
    "\n",
    "$$ \\delta m = \\sqrt{\\frac{1}{Z} \\sum_{i=1}^N \\frac{1}{(\\delta y_i)^2}},$$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\delta b = \\sqrt{\\frac{1}{Z} \\sum_{i=1}^N \\frac{x_i^2}{(\\delta y_i)^2}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242111d-660a-4cd2-8454-90933811188d",
   "metadata": {},
   "source": [
    "## 8.3 Applying the two-parameter analytic equations to sample data\n",
    "\n",
    "The following code uses our common format to show how to implement these analytic best-fit equations, coming from chi-squared minimization. They use the common formatting we have seen of updating the plotting variables and defining labels and titles at the start to make it as easy as possible to use this code with a new data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69db76cc-a6db-4cbc-ae27-a856df63e554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample data (arbitrary units).\n",
    "x_sample_data = np.array([\n",
    "    0.1, 0.16428571, 0.22857143, 0.29285714, 0.35714286, \n",
    "    0.42142857, 0.48571429, 0.55, 0.61428571, 0.67857143, \n",
    "    0.74285714, 0.80714286, 0.87142857, 0.93571429, 1.\n",
    "    ])\n",
    "y_sample_data = np.array([\n",
    "    0.33336864, 0.5414786, 0.82003978, 1.09858314, 1.27560974, \n",
    "    1.52025082, 1.67681586, 2.03833678, 2.35943739, 2.36120224, \n",
    "    2.74941308, 2.83963194, 2.9932707, 3.40978616, 3.44578725\n",
    "    ])\n",
    "del_y_sample_data = np.array([\n",
    "    0.01666843, 0.02707393, 0.04100199, 0.05492916, 0.06378049, \n",
    "    0.07601254, 0.08384079, 0.10191684, 0.11797187, 0.11806011, \n",
    "    0.13747065, 0.1419816, 0.14966353, 0.17048931, 0.17228936\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a1d716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR MODEL + ANALYTIC MINIMIZATION OF REDUCED CHI-SQUARE\n",
    "\n",
    "# Model – step 1: find the range of x values from the experimental data.\n",
    "x_data = x_sample_data\n",
    "y_data = y_sample_data\n",
    "del_y_data = del_y_sample_data\n",
    "x_min = np.min(x_data)  # find the smallest x value\n",
    "x_max = np.max(x_data)  # find the largest x value\n",
    "\n",
    "# Model – step 2: generate an array of model x values between x_min and x_max\n",
    "# for which we want to plot the model y values.\n",
    "x_model = np.linspace(\n",
    "    start=x_min, stop=x_max, num=200\n",
    "    )  # return 200 evenly spaced values\n",
    "\n",
    "# Model – step 3: calculate the model y values at each of the model x values.\n",
    "# Analytical calculation of best-fit parameter values and their uncertainty for\n",
    "# a linear model: y = mx + b.\n",
    "\n",
    "# Calculate the placeholder variable Z.\n",
    "z = (\n",
    "    np.sum(1 / del_y_data**2) * np.sum(x_data**2 / del_y_data**2)\n",
    "    - np.sum(x_data / del_y_data**2)**2\n",
    "    )\n",
    "\n",
    "# Calculate the best-fit value for the slope, m.\n",
    "m = 1 / z * (\n",
    "    np.sum(1 / del_y_data**2) * np.sum(x_data * y_data / del_y_data**2)\n",
    "    - np.sum(x_data / del_y_data**2) * np.sum(y_data / del_y_data**2)\n",
    "    )\n",
    "# Calculate the uncertainty in the slope, del_m.\n",
    "del_m = np.sqrt(1 / z * np.sum(1 / del_y_data**2))\n",
    "\n",
    "# Calculate the best-fit value for the y-intercept, b.\n",
    "b = 1 / z * (\n",
    "    np.sum(x_data**2 / del_y_data**2) * np.sum(y_data / del_y_data**2)\n",
    "    - np.sum(x_data / del_y_data**2) * np.sum(x_data * y_data / del_y_data**2)\n",
    "    )\n",
    "# Calculate the uncertainty in the y-intercept, del_b.\n",
    "del_b = np.sqrt(1 / z * np.sum(x_data**2 / del_y_data**2))\n",
    "\n",
    "# Calculate the values predicted by the model.\n",
    "y_model = m * x_model + b\n",
    "\n",
    "# Model – step 4: plot the model on the graph of the experimental data.\n",
    "# Create a figure and a set of subplots and reference them in the\n",
    "# variables \"fig\" and \"axs\".\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=2, ncols=1, squeeze=False,\n",
    "    height_ratios=[1.75, 1], figsize=(6, 8)\n",
    "    )  # do not change\n",
    "data_label = \"sample data\"\n",
    "graph_title = \"Data with the analytic best-fitting curve from a linear model\"\n",
    "x_label = \"$x$ (arbitrary units)\"  # adapt to your experiment\n",
    "y_label = \"$y$ (arbitrary units)\"  # adapt to your experiment\n",
    "axs[0, 0].errorbar(\n",
    "    x=x_data, y=y_data, yerr=del_y_data,\n",
    "    fmt='bo', markersize=3, label=data_label\n",
    "    )  # plot experimental data\n",
    "axs[0, 0].set_title(graph_title)\n",
    "axs[0, 0].set_xlabel(x_label)\n",
    "axs[0, 0].set_ylabel(y_label)\n",
    "axs[0, 0].grid()  # add a grid\n",
    "\n",
    "model_label = \"model ($y = mx + b$)\"\n",
    "axs[0, 0].plot(x_model, y_model, \"r-\", label=model_label)  # plot model data\n",
    "# Add a legend (you can change the location as needed)\n",
    "axs[0, 0].legend(loc='upper left')\n",
    "\n",
    "# Residuals – step 1: calculate the model predictions y_prediction for each of\n",
    "# the measured x_data values.\n",
    "y_prediction = m * x_data + b\n",
    "\n",
    "# Residuals – step 2: calculate the residuals.\n",
    "residuals = y_data - y_prediction\n",
    "\n",
    "# Residuals – step 3: plot the residuals against the measured x_data values.\n",
    "residual_graph_title = \"Corresponding residuals\"\n",
    "residual_y_label = \"residual = data - model (arbitrary units)\"\n",
    "axs[1, 0].errorbar(\n",
    "    x=x_data, y=residuals, yerr=del_y_data,\n",
    "    fmt='bo', markersize=3, label=data_label\n",
    "    )\n",
    "axs[1, 0].set_title(residual_graph_title)\n",
    "axs[1, 0].set_xlabel(x_label)  # reuse the x-label from the scatter plot\n",
    "axs[1, 0].set_ylabel(residual_y_label)\n",
    "axs[1, 0].grid()  # add a grid\n",
    "\n",
    "# Residuals – step 4: add a horizontal line at r=0 to the plot.\n",
    "axs[1, 0].hlines(y=0, xmin=x_min, xmax=x_max, color='k', label=\"$r = 0$\")\n",
    "# Add a legend (you can change the location as needed)\n",
    "axs[1, 0].legend(loc='lower left', fontsize='small')\n",
    "\n",
    "plt.tight_layout()  # adjust the padding between and around subplots\n",
    "plt.show()\n",
    "\n",
    "# Summary of the reduced chi-square analysis.\n",
    "N = len(x_data)\n",
    "\n",
    "# Define the number of fitting parameters (\"P\" in the reduced chi-square\n",
    "# formula).\n",
    "number_params = 2  # slope and y-intercept\n",
    "\n",
    "# Calculate the reduced chi-square.\n",
    "reduced_chi_square = 1 / (len(residuals) - number_params) * np.sum(\n",
    "    (residuals / del_y_data)**2\n",
    "    )\n",
    "\n",
    "# Advanced code for creating a fit report in Python with the right number of \n",
    "# decimals (for the best value) and significant figures (for the uncertainty).\n",
    "\n",
    "# Note that in the printed report, the number in parentheses next to the\n",
    "# uncertainty in the parameters is the associated relative uncertainty\n",
    "# (expressed as a percentage).\n",
    "\n",
    "m_units = \"arbitrary units\"  # adapt to your experiment\n",
    "b_units = \"arbitrary units\"  # adapt to your experiment\n",
    "\n",
    "# You should not need to modify this part of the code below.\n",
    "SIG_FIGS = 2  # constant in our class\n",
    "\n",
    "print(f\"{del_m:#.{SIG_FIGS}g}\")\n",
    "print(\"FIT REPORT\")\n",
    "print(\"[Model]\")\n",
    "print(f\"{'':<4}\"\"Linear model: y = mx + b\")\n",
    "print(\"[Fit Statistics]\")\n",
    "print(f\"{'':<4}{'# fitting method':<18}: analytic chi-square minimization\")\n",
    "print(f\"{'':<4}{'# data points':<18}{': N = ' + str(N):<5}\")\n",
    "print(f\"{'':<4}{'# parameters':<18}{': P = ' + str(number_params):<5}\")\n",
    "print(f\"{'':<4}\"f\"reduced chi-square = {reduced_chi_square:.{SIG_FIGS}f}\")\n",
    "print(\"[Parameters (best-fitting values)]\")\n",
    "print(\n",
    "    f\"{'':<4}{'slope: m = ':>17}{m:< #12g}{' ± ':<3}\"\n",
    "    f\"{del_m:< #12g} {m_units:<15} \"\n",
    "    f\"({abs(del_m / m):.{SIG_FIGS}%})\"\n",
    "    )\n",
    "print(\n",
    "    f\"{'':<4}{'y-intercept: b = ':>17}{b:< #12g}{' ± ':<3}\"\n",
    "    f\"{del_b:< #12g} {b_units:<15} \"\n",
    "    f\"({abs(del_b / b):.{SIG_FIGS}%})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076950fe-1591-441e-9a0c-23e8e735ad36",
   "metadata": {},
   "source": [
    "### Discussion of goodness of fit using the residuals plot and chi-squared\n",
    "\n",
    "Let's take a moment to ask ourselves if this is a good fit. \n",
    "\n",
    "If we look first at the residuals, we see that there are no obvious trends and the residuals seem to be distributed randomly and evenly above and below the Residuals = 0 line. We also see an appropriate number of the residual uncertainties crossing this Residuals = 0 line. The residuals graph suggests we have a pretty good fit. This is further reinforced by our chi-squared of 0.62, which also suggests we have a good fit. These reasonable residuals, along with $\\chi^2 \\approx 1$, allow us to conclude that, overall, we have a good fit of this model to these data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c6eb3-060f-4ffa-bd57-0e0f8f989ca9",
   "metadata": {},
   "source": [
    "## 8.4 Sidebar: Visualizing chi-squared minimization\n",
    "\n",
    "***This section is a supplemental sidebar intended to provide a visualization of how chi-squared varies as you change slope away from the best-fit slope. The code from here is not intended to be used in your lab notebook as it is provided just for the purpose of giving an example visualization of finding a minimum for chi-squared.***\n",
    "\n",
    "Below, you can see a visualization of how the analytically-determined $m$ lies at the lowest point of the chi-squared vs. $m$ curve (as expected for a minimum). We fix $b$ as our best fit value and then use an array of many different $m$ values, with chi-squared calculated for each of these different slope values. Finally, the calculated chi-squared values are plotted versus the slope values.\n",
    "\n",
    "The code below uses a programming construction of Python called a \"for\" loop. You will not need to know how to use these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5d8540-6a0a-469d-8f0b-a1c3462aa24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me to see a plot of chi-squared vs slope, for slopes ranging from\n",
    "# mMin to mMax\n",
    "# - This code is for information only and not intended to be used in\n",
    "# your lab notebook\n",
    "\n",
    "\"\"\" plot chi-squared as a function of m \"\"\"\n",
    "# prepare an array of different slope values between mMin and mMax.\n",
    "mVec = np.linspace(m - del_m, m + del_m, 100)\n",
    "# create an array of chi-squared values, set each to 0 for now. \n",
    "reduced_chi_squareVec = np.zeros(np.size(mVec))\n",
    "\n",
    "for i in range(len(mVec)):  # loop through all the different m values.\n",
    "    # This indented code is executed once for each of the m values \n",
    "    # we calculate chi-squared for each possible slope.\n",
    "    # Calculate model for the current value of m in the vector\n",
    "    ymodelTemp = mVec[i] * x_data + b\n",
    "    resTemp = y_data - ymodelTemp  # residuals for this model\n",
    "    wres2Temp = (resTemp / del_y_data)**2  # weighting these residuals\n",
    "    # Store chi2 in the ith value of chi2Vec.\n",
    "    reduced_chi_squareVec[i] = np.sum(wres2Temp) / (N - number_params)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(mVec, reduced_chi_squareVec, 'k')\n",
    "plt.plot(m, reduced_chi_square, 'o', label='best fit')\n",
    "plt.xlabel('$m$')\n",
    "plt.ylabel('Reduced chi-square')\n",
    "plt.title(\n",
    "    f'Visualizing chi-squared minimization for various values of $m$,'\n",
    "    f'\\n with a fixed intercept of $b =${b:.3f}'\n",
    "    )\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a58820-221e-46dd-9675-9052f2e14c0b",
   "metadata": {},
   "source": [
    "## 8.5 Prepare for Lab 08"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5cd5f7-2f56-4829-bb14-58187c9acd32",
   "metadata": {},
   "source": [
    "In Lab 08, you will use the two-parameter analytic equations derived above to calculate the best-fitting parameters (and uncertainty) for the model describing your $\\tau$ versus $R$ data. \n",
    "\n",
    "**Your turn #2:** In preparation for Lab 08, adapt the code \"# LINEAR MODEL + ANALYTIC MINIMIZATION OF REDUCED CHI-SQUARE\" from 8.3 above to your Lab 07 data. Check that this gives you a lower $\\chi^2_\\text{red}$ than what you got from minimizing by hand in Lab 07."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89559f10-928a-4540-8bd8-5ae30f1f0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a copy of your Lab 07 data.\n",
    "# de1 = data_entry2.sheet_copy('../Lab07/lab07_round2','copy-lab07_round2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f799be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the content of this Python cell by the adapted code from 8.3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08afeee-7cb1-4807-8668-e30d7abb0d85",
   "metadata": {},
   "source": [
    "# Appendix - The full derivation of the analytic formula for minimizing chi-squared for the one-parameter model, $y=mx$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bde226e-3b00-451d-ae48-be71f10f04bc",
   "metadata": {},
   "source": [
    "From your calculus courses, you know that you can take a continuous function $f(x)$ and find the minimum or maximum (a critical point, $x_c$) by taking the derivative of $f(x)$ with respect to $x$, setting this derivative equal to zero, and solving the resulting equation\n",
    "\n",
    "$$\\left[\\frac{df(x)}{dx}\\right]_{x=x_c}=0.$$\n",
    "\n",
    "Since $\\chi^2$ is a continuous function, we can do exactly this process to come up with an expression that automatically calculates the critical point(s), to find the minimum. In other words, we can use calculus to derive an *analytic* expression for the best fit parameter(s).\n",
    "\n",
    "*NOTE! for those thinking: wait a minute, how do we know the critical point will be a minimum and not a maximum? If we think about fitting a model to data that is finite, we know that as we move the parameters to $+\\infty$ and $-\\infty$ the fit will become increasingly worse to the data, meaning that chi-squared has no maxima. From this observation is follows that any critical point found must be a minimum.*\n",
    "\n",
    "In the simplest case of a one-parameter linear model, $y=mx$, we wish to minimize chi-squared with respect to $m$ to find the best fit slope\n",
    "\n",
    "$$ \\frac{d\\chi^2(m)}{dm} = 0.$$\n",
    "\n",
    "We can first substitute our general expression for chi-squared\n",
    "\n",
    "$$ \\frac{d}{dm}\\left[ \\frac{1}{N-P} \\sum_{i=1}^N \\left(\\frac{y_i - f(x_i)}{\\delta y_i}\\right)^2 \\right] = 0.$$\n",
    "\n",
    "Our model is $f(x_i) = mx_i$, which we can substitute into the above expression\n",
    "\n",
    "$$ \\frac{d}{dm}\\left[ \\frac{1}{N-P} \\sum_{i=1}^N \\left(\\frac{y_i - mx_i}{\\delta y_i}\\right)^2 \\right] = 0.$$\n",
    "\n",
    "Since the derivative is with respect to $m$, it has no effect on $N$ or $P$, meaning we can move that leading fraction outside of the derivative.\n",
    "\n",
    "$$ \\frac{1}{N-P} \\frac{d}{dm} \\sum_{i=1}^N \\left(\\frac{y_i - mx_i}{\\delta y_i}\\right)^2 = 0.$$\n",
    "\n",
    "The summation is only over variables with a subscript \"$i$\"; $m$ does not contain this so we can also switch the order of differentiation and summation\n",
    "\n",
    "$$ \\frac{1}{N-P} \\sum_{i=1}^N \\frac{d}{dm} \\left(\\frac{y_i - mx_i}{\\delta y_i}\\right)^2 = 0.$$\n",
    "\n",
    "Now we perform some calculus and take the derivative (invoking the chain rule)\n",
    "\n",
    "$$ \\frac{2}{N-P} \\sum_{i=1}^N  \\left(\\frac{y_i - mx_i}{\\delta y_i}\\right) \\cdot \\frac{d}{dm} \\left(\\frac{y_i - mx_i}{\\delta y_i}\\right)= 0,$$\n",
    "\n",
    "$$ \\frac{2}{N-P} \\sum_{i=1}^N  \\left(\\frac{y_i - mx_i}{\\delta y_i}\\right) \\cdot \\left(-\\frac{x_i}{\\delta y_i}\\right) = 0.$$\n",
    "\n",
    "The negative sign can be taken outside the sum, and since we are setting everything equal to zero the $2/(N-P)$ can be discarded\n",
    "\n",
    "$$ \\sum_{i=1}^N  \\left(\\frac{y_i - mx_i}{\\delta y_i}\\right) \\cdot \\frac{x_i}{\\delta y_i} = 0.$$\n",
    "\n",
    "What remains is to rearrange this expression for $m$. We can start by expanding the terms in the summation\n",
    "\n",
    "$$ \\sum_{i=1}^N  \\left(\\frac{y_i}{\\delta y_i} - m\\frac{x_i}{\\delta y_i}\\right) \\cdot \\frac{x_i}{\\delta y_i} = 0$$\n",
    "$$ \\sum_{i=1}^N  \\frac{x_iy_i}{(\\delta y_i)^2} - m\\frac{x_i^2}{(\\delta y_i)^2} = 0$$\n",
    "\n",
    "then finally isolate $m$\n",
    "\n",
    "$$ m = \\frac{\\displaystyle \\sum_{i=1}^N  \\frac{x_iy_i}{(\\delta y_i)^2}}{\\displaystyle \\sum_{i=1}^N \\frac{x_i^2}{(\\delta y_i)^2}} $$\n",
    "\n",
    "So given our $x$ and $y$ data plus the uncertainty in $y$, we are able to analytically solve for the best fit slope using this equation! The uncertainty in this slope can be determined from the uncertainties in the data by uncertainty propagation. The result is:\n",
    "\n",
    "$$ \\delta m = \\sqrt{\\frac{1}{\\displaystyle \\sum_{i=1}^N \\frac{x_i^2}{(\\delta y_i)^2}}} .$$\n",
    "\n",
    "Since the term $\\sum_{i=1}^N \\frac{x_i^2}{(\\delta y_i)^2}$ appears both in $m$ and $\\delta m$, it is convenient for notation and coding purposes to define a placeholder variable\n",
    "\n",
    "$$ Z = \\sum_{i=1}^N \\frac{x_i^2}{(\\delta y_i)^2} $$\n",
    "\n",
    "such that\n",
    "\n",
    "$$ m = \\frac{1}{Z} \\sum_{i=1}^N  \\frac{x_iy_i}{(\\delta y_i)^2} $$\n",
    "$$ \\delta m = \\sqrt{\\frac{1}{Z}} .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193db82c-8f80-4c7b-8c06-3323a4d98aa0",
   "metadata": {},
   "source": [
    "# Submit\n",
    "\n",
    "Steps for submission:\n",
    "\n",
    "1. Click: Run => Run_All_Cells\n",
    "2. Read through the notebook to ensure all the cells executed correctly and without error.\n",
    "3. File => Save_and_Export_Notebook_As->HTML\n",
    "4. Inspect your exported html file.\n",
    "5. Upload the HTML document to the lab submission assignment on Canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phys119",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
