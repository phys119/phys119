{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0607874-fa9a-44d1-a7af-2c5db485891a",
   "metadata": {},
   "source": [
    "# Lab 08 Prelab: Analytic chi-squared minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570b70d5-9afc-45b0-bd6a-56ba39311b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import data_entry2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c7a22-ebd3-4977-9d87-395137731c65",
   "metadata": {},
   "source": [
    "In Lab 08 you will continue to collect data from the RC circuit in an effort to improve your dataset. Additionally, this prelab will provide you with an analytic formula to help you automatically **calculate** the fit parameters that correspond to the best possible fit of your model to your data.\n",
    "\n",
    "**An important reminder: the \"best fit\" is not the same as a \"good fit\".** Even though the analytic formula will give you the best fit of your model to your data, it does not allow you to say \"this is a *good* fit\". The best fit, by definition, is one that minimizes chi-squared, but a minimized chi-squared of $\\chi^2=11.8$ or one with a strong pattern in the residuals is still not considered a good fit to your data. Conversely, a minimized chi-squared of $\\chi^2=0.05$ still requires you to take a closer look at your uncertainty estimation strategy, since the uncertainties would likely have been overestimated. This automated method of chi-squared minimization still requires you to use residuals plots and your other techniques for assessing goodness of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d44b16-605b-4d2c-b5f6-a5679404bc55",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The analytic formula for minimizing chi-squared for a one-parameter model, $y = mx$, with no y-intercept\n",
    "_This section details deriving the analytic formula for minimizing chi-squared for the one-parameter model, $y = mx$, while the next section provides the solution to the two-parameter model, $y = mx + b$. The solution for the two-parameter model is the one we will be using in the lab, but it is much more complicated so we start with the one-parameter model as an intermediate step._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ee9a5-d5b5-47bf-b100-b396faa1bc8e",
   "metadata": {},
   "source": [
    "Recall that chi-squared is a continuous function of the fitting parameters in your model, meaning that if you change one of the fitting parameters (such as increasing or decreasing the slope), this will change the resulting value of chi-squared. If you have $P$ parameters in your model, then chi-squared is a $P$-dimensional function. For instance, if we are fitting a one-parameter linear model, $y=mx$, then $m$ is the sole parameter and the associated chi-squared function is $\\chi^2(m)$. For a two-parameter linear model, $y=mx+b$, then we would have $\\chi^2(m,b)$. \n",
    "\n",
    "For a simple model function like a straight line, we can use calculus to analytically find the parameter values that minimize chi-squared instead of manually adjusting the parameters like we have been doing so far. Below, we'll work through that calculation for the case of a straight-line fit with no intercept. We work through each step in this derivation to make the process clear.\n",
    "\n",
    "From your calculus courses, you know that you can take a continuous function $f(x)$ and find the minimum or maximum in that function (a critical point, $x_c$) by taking the derivative of $f(x)$ with respect to $x$, setting this derivative equal to zero, and then solving the resulting equation:\n",
    "\n",
    "$$\\left[\\frac{df(x)}{dx}\\right]_{x=x_c}=0.$$\n",
    "\n",
    "Since $\\chi^2$ is a continuous function, we can follow this process to come up with an expression that automatically calculates the critical point(s), to find the minimum. In other words, we can use calculus to derive an *analytic* expression for the best fit parameter(s).\n",
    "\n",
    "*NOTE! for those thinking: wait a minute, how do we know the critical point will be a minimum and not a maximum? If we think about fitting a model to data, we know that as we move the parameters to $+\\infty$ and $-\\infty$ the fit will become increasingly worse, meaning that chi-squared has no maxima. From this observation is follows that any critical point found must be a minimum.*\n",
    "\n",
    "In the simplest case of a one-parameter linear model, $y=mx$, we wish to minimize chi-squared with respect to $m$ to find the best-fit slope,\n",
    "\n",
    "$$ \\frac{d\\chi^2(m)}{dm} = 0.$$\n",
    "\n",
    "We can first substitute our general expression for chi-squared\n",
    "\n",
    "$$ \\frac{d}{dm}\\left[ \\frac{1}{N-P} \\sum_{i=1}^N \\left(\\frac{y_i - f(x_i)}{\\delta y_i}\\right)^2 \\right] = 0.$$\n",
    "\n",
    "Our model is $f(x_i) = mx_i$, which we can substitute into the above expression\n",
    "\n",
    "$$ \\frac{d}{dm}\\left[ \\frac{1}{N-P} \\sum_{i=1}^N \\left(\\frac{y_i - mx_i}{\\delta y_i}\\right)^2 \\right] = 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a0ea8-14ed-41f6-bc95-eb3aa7aee936",
   "metadata": {},
   "source": [
    "If we differentiate the above and solve for $m$, we will find the slope ($m$) that corresponds to the lowest possible chi-squared. Here, we skip ahead to the solution, but include the full derivation in an appendix at the end of this document.\n",
    "\n",
    "$$ m = \\frac{\\displaystyle \\sum_{i=1}^N  \\frac{x_i y_i}{(\\delta y_i)^2}}{\\displaystyle \\sum_{i=1}^N \\frac{x_i^2}{(\\delta y_i)^2}}.$$\n",
    "\n",
    "*Note: if you look at how this equation was written in Markdown, you can see that we are using display-style notation to make the summation symbols and fractions align more aesthetically.*\n",
    "\n",
    "Given our $x$ and $y$ data (including uncertainty in $y$), we are able to analytically solve for the best-fit slope using this equation! The uncertainty in this slope can be determined from the uncertainties in the data by uncertainty propagation. The result is:\n",
    "\n",
    "$$ \\delta m = \\sqrt{\\frac{1}{\\displaystyle \\sum_{i=1}^N \\dfrac{x_i^2}{(\\delta y_i)^2}}} .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e8a627-0ea4-46a4-85da-1ec6cde33ba8",
   "metadata": {},
   "source": [
    "Since the term $\\sum_{i=1}^N \\frac{x_i^2}{(\\delta y_i)^2}$ appears both in $m$ and $\\delta m$, it is convenient for notation and coding purposes to define a placeholder variable\n",
    "\n",
    "$$ Z = \\sum_{i=1}^N \\dfrac{x_i^2}{(\\delta y_i)^2},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23059221-5598-442b-aa3b-b9a5397e0a90",
   "metadata": {},
   "source": [
    "such that\n",
    "\n",
    "$$m = \\dfrac{1}{Z} \\sum_{i=1}^N  \\dfrac{x_iy_i}{(\\delta y_i)^2},$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\delta m = \\sqrt{\\dfrac{1}{Z}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d610e4-5217-4038-8147-555422212c01",
   "metadata": {},
   "source": [
    "**Your turn #1:** Take a close look at the expression for $\\delta m$. How do (A) the number of data points, $N$, and (B) the uncertainies in the data, $\\delta y_i$, impact $\\delta m$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad46f59-2db0-4447-acea-cd032c4b3553",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0e4954c-f95d-42bc-9731-965a015f465b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### **Uncollapse for answer to Your Turn #1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cce2d45-9200-4c46-82e9-e64e2aa5287b",
   "metadata": {},
   "source": [
    "a) $Z$ is a sum over each data point, without a 1/$N$ term, so each additional point will always increase $Z$. Since $Z$ is in the denominator, $\\delta m$ will get smaller for each additional data point added.\n",
    "\n",
    "b) Similarly, the smaller $\\delta y$, the larger $Z$, and thus the smaller $\\delta m$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec2bd40-54e0-4857-9849-5d585ad7b4f1",
   "metadata": {},
   "source": [
    "## The analytic formulae for minimizing chi-squared for the two-parameter model, $y = mx + b$\n",
    "In this section we jump straight into the analytic formulas for for minimizing chi-squared for the two-parameter model, $y = mx + b$. The process is essentially the same as for the one-parameter model above, except we will now also find $b$ by differerntiating chi-squared with respect to $b$ and setting that to 0 to find the minimum. Our initial expressions are then as follows:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial m}\\left[ \\frac{1}{N-P} \\sum_{i=1}^N \\left(\\frac{y_i - (mx_i + b)}{\\delta y_i}\\right)^2 \\right] = 0,$$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial b}\\left[ \\frac{1}{N-P} \\sum_{i=1}^N \\left(\\frac{y_i - (mx_i + b)}{\\delta y_i}\\right)^2 \\right] = 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c284f1-c025-4dd0-936a-a87d51779ec2",
   "metadata": {},
   "source": [
    "After differentiation and some algebra, we find that the solutions for a weighted fit to the two-parameter model $y=mx + b$ are\n",
    "\n",
    "$$m = \\frac{1}{Z} \\left(\n",
    "\\sum_{i=1}^N \\frac{1}{(\\delta y_i)^2} \\cdot \\sum_{i=1}^N \\frac{x_i y_i}{(\\delta y_i)^2} \n",
    "- \\sum_{i=1}^N \\frac{x_i}{(\\delta y_i)^2} \\cdot \\sum_{i=1}^N \\frac{y_i}{(\\delta y_i)^2}\n",
    "\\right),$$\n",
    "\n",
    "and\n",
    "\n",
    "$$b = \\frac{1}{Z} \\left(\n",
    "\\sum_{i=1}^N \\frac{x_i^2}{(\\delta y_i)^2} \\cdot \\sum_{i=1}^N \\frac{y_i}{(\\delta y_i)^2} \n",
    "- \\sum_{i=1}^N \\frac{x_i}{(\\delta y_i)^2} \\cdot \\sum_{i=1}^N \\frac{x_i y_i}{(\\delta y_i)^2}\n",
    "\\right),$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f6a335-649d-4216-9c79-aac7494688ca",
   "metadata": {},
   "source": [
    "where $Z$ is a placeholder variable (different from the 1-parameter version) defined as\n",
    "\n",
    "$$ Z = \\sum_{i=1}^N \\frac{1}{(\\delta y_i)^2} \\cdot \\sum_{i=1}^N \\frac{x_i^2}{(\\delta y_i)^2}\n",
    "- \\left( \\sum_{i=1}^N \\frac{x_i}{(\\delta y_i)^2} \\right)^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c94761-3caa-4bc3-be14-e0300d3450c4",
   "metadata": {},
   "source": [
    "The uncertainty in the fit parameters are given by\n",
    "\n",
    "$$ \\delta m = \\sqrt{\\frac{1}{Z} \\sum_{i=1}^N \\frac{1}{(\\delta y_i)^2}},$$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\delta b = \\sqrt{\\frac{1}{Z} \\sum_{i=1}^N \\frac{x_i^2}{(\\delta y_i)^2}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242111d-660a-4cd2-8454-90933811188d",
   "metadata": {},
   "source": [
    "## Applying the two-parameter analytic equations to sample data\n",
    "\n",
    "The following code uses our common format to show how to implement these analytic best-fit equations, coming from chi-squared minimization. They use the common formatting we have seen of updating the plotting variables and defining labels and titles at the start to make it as easy as possible to use this code with a new data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69db76cc-a6db-4cbc-ae27-a856df63e554",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Sample data for the code below \"\"\"\n",
    "x_sample_data = np.array([0.1, 0.16428571, 0.22857143, 0.29285714, 0.35714286, \n",
    "                  0.42142857, 0.48571429, 0.55, 0.61428571, 0.67857143, \n",
    "                  0.74285714, 0.80714286, 0.87142857, 0.93571429, 1.])\n",
    "y_sample_data = np.array([0.33336864, 0.5414786, 0.82003978, 1.09858314, 1.27560974, \n",
    "                  1.52025082, 1.67681586, 2.03833678, 2.35943739, 2.36120224, \n",
    "                  2.74941308, 2.83963194, 2.9932707, 3.40978616, 3.44578725])\n",
    "dy_sample_data = np.array([0.01666843, 0.02707393, 0.04100199, 0.05492916, 0.06378049, \n",
    "                    0.07601254, 0.08384079, 0.10191684, 0.11797187, 0.11806011, \n",
    "                    0.13747065, 0.1419816, 0.14966353, 0.17048931, 0.17228936])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4746ff4d-ae58-49f4-97ef-7e500441ce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 2-parameter analytic best-fit solution and plotting \"\"\"\n",
    "\n",
    "# Define the variables we will be plotting\n",
    "xdata = x_sample_data\n",
    "ydata = y_sample_data\n",
    "dydata = dy_sample_data\n",
    "\n",
    "# Labels and titles\n",
    "data_label = \"Sample data\"\n",
    "model_label = \"y = mx + b\"\n",
    "graph_title = \"The best-fit line using the two-parameter analytic equations\"\n",
    "x_label = \"Sample x-data (units)\"\n",
    "y_label = \"Sample y-data (units)\"\n",
    "residuals_y_label = \"Residual = data - model (units)\"\n",
    "\n",
    "\"\"\" Find the best 2-parameter model corresponding to the minimized chi-squared \"\"\"\n",
    "\n",
    "# Calculate Z\n",
    "Z = (\n",
    "    np.sum( 1 / dydata**2 ) * np.sum( xdata**2 / dydata**2 )\n",
    "    - np.sum( xdata / dydata**2 )**2\n",
    ")\n",
    "\n",
    "# Calculate best fit slope, m\n",
    "m = 1/Z * (\n",
    "    np.sum( 1 / dydata**2 ) * np.sum( xdata * ydata / dydata**2 )\n",
    "    - np.sum( xdata / dydata**2 ) * np.sum( ydata / dydata**2 )\n",
    ")\n",
    "\n",
    "# Calculate best fit y-intercept, b\n",
    "b = 1/Z * (\n",
    "    np.sum( xdata**2 / dydata**2 ) * np.sum( ydata / dydata**2 )\n",
    "    - np.sum( xdata / dydata**2 ) * np.sum( xdata * ydata / dydata**2 )\n",
    ")\n",
    "\n",
    "# Calculate uncertainty in best fit slope, dm\n",
    "dm = np.sqrt(1/Z * np.sum( 1 / dydata**2 ) )\n",
    "\n",
    "# Calculate uncertainty in best fit y-intercept, db\n",
    "db = np.sqrt(1/Z * np.sum( xdata**2 / dydata**2 ) )\n",
    "\n",
    "# Print the best fit slope and uncertainty\n",
    "print(\"Best fit slope, m = \", m, \"±\", dm)\n",
    "\n",
    "# Print the best fit y-intercept and uncertainty\n",
    "print(\"Best fit y-intercept, b = \", b, \"±\", db)\n",
    "\n",
    "\n",
    "\"\"\" Construct the model for plotting and calculating residuals \"\"\"\n",
    "\n",
    "ymodel = m * xdata + b # best fit model\n",
    "res = ydata - ymodel # calculate residuals (best fit)\n",
    "wres2 = (res/dydata)**2 # weighted residuals squared\n",
    "\n",
    "\n",
    "\"\"\" Calculate chi-squared \"\"\"\n",
    "    \n",
    "N = len(xdata) # number of data points\n",
    "P = 2 # number of parameters in y = mx + b\n",
    "chi2 = np.sum(wres2) / (N - P) # calculate chi-squared\n",
    "print(f\"chi2 = {chi2:.4f}\")\n",
    "\n",
    "\"\"\" Plot data and fit \"\"\"\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(xdata, ydata, dydata, marker='.', linestyle='', color='b', label = data_label)\n",
    "plt.plot(xdata, ymodel, color='r', label=model_label)\n",
    "plt.xlabel(x_label)\n",
    "plt.ylabel(y_label)\n",
    "plt.title(graph_title)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\"\"\" Plot residuals for the best fit \"\"\"\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(xdata, res, dydata, marker='.', linestyle='', color='b')\n",
    "plt.hlines(y=0, xmin=np.min(xdata), xmax=np.max(xdata), color='k') # draw axis at y = 0.\n",
    "plt.xlabel(x_label)\n",
    "plt.ylabel(residuals_y_label)\n",
    "plt.title(f'Residuals plot (best fit, $\\chi^2$={chi2:.4f})')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076950fe-1591-441e-9a0c-23e8e735ad36",
   "metadata": {},
   "source": [
    "### Discussion of goodness of fit using the residuals plot and chi-squared\n",
    "\n",
    "Let's take a moment to ask ourselves if this is a good fit. \n",
    "\n",
    "If we look first at the residuals, we see that there are no obvious trends and the residuals seem to be distributed randomly and evenly above and below the Residuals = 0 line. We also see an appropriate number of the residual uncertainties crossing this Residuals = 0 line. The residuals graph suggests we have a pretty good fit. This is further reinforced by our chi-squared of 0.62, which also suggests we have a good fit. These reasonable residuals, along with $\\chi^2 \\approx 1$, allow us to conclude that, overall, we have a good fit of this model to these data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c6eb3-060f-4ffa-bd57-0e0f8f989ca9",
   "metadata": {},
   "source": [
    "## Sidebar: Visualizing chi-squared minimization\n",
    "\n",
    "***This section is a supplemental sidebar intended to provide a visualization of how chi-squared varies as you change slope away from the best-fit slope. The code from here is not intended to be used in your lab notebook as it is provided just for the purpose of giving an example visualization of finding a minimum for chi-squared.***\n",
    "\n",
    "Below, you can see a visualization of how the analytically-determined $m$ lies at the lowest point of the chi-squared vs. $m$ curve (as expected for a minimum). We fix $b$ as our best fit value and then use an array of many different $m$ values, with chi-squared calculated for each of these different slope values. Finally, the calculated chi-squared values are plotted versus the slope values.\n",
    "\n",
    "The code below uses a programming construction of Python called a \"for\" loop. You will not need to know how to use these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5d8540-6a0a-469d-8f0b-a1c3462aa24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me to see a plot of chi-squared vs slope, for slopes ranging from mMin to mMax\n",
    "# - This code is for information only and not intended to be used in your lab notebook\n",
    "\n",
    "\"\"\" plot chi-squared as a function of m \"\"\"\n",
    "\n",
    "mVec = np.linspace(m-dm,m+dm,100) # prepare an array of different slope values between mMin and mMax.\n",
    "chi2Vec = np.zeros(np.size(mVec)) # create an array of chi-squared values, set each to 0 for now.\n",
    "\n",
    "for i in range(len(mVec)): # loop through all the different m values.\n",
    "    \n",
    "    # This indented code is executed once for each of the m values \n",
    "    # we calculate chi-squared for each possible slope.\n",
    "    ymodelTemp = mVec[i]*xdata + b # model for the current value of m in the vector\n",
    "    resTemp = ydata - ymodelTemp # residuals for this model\n",
    "    wres2Temp = (resTemp / dydata)**2 # weighting these residuals\n",
    "    chi2Vec[i] = np.sum(wres2Temp) / (N - P) # store chi2 in the ith value of chi2Vec.\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(mVec, chi2Vec, 'k')\n",
    "plt.plot(m, chi2, 'o', label='best fit')\n",
    "plt.xlabel('m')\n",
    "plt.ylabel('Chi-squared')\n",
    "plt.title(f'Visualizing chi-squared minimization for various values of $m$,\\nwith a fixed intercept of $b =$ {b:.3f}')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a58820-221e-46dd-9675-9052f2e14c0b",
   "metadata": {},
   "source": [
    "## Prepare for Lab 08"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5cd5f7-2f56-4829-bb14-58187c9acd32",
   "metadata": {},
   "source": [
    "In Lab 08, you will use the analytic formula derived above to calculate the best fit slope for your time-constant versus resistance data. \n",
    "\n",
    "**Your turn #2:** In preparation for Lab 08, adapt your calculations from above for your Lab 07 data. Check that this gets you a lower chi-squared than what you got from minimizing by hand in Lab 07."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89559f10-928a-4540-8bd8-5ae30f1f0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to re-analyze your Lab 07 data using your \n",
    "# python calculations from the analytic formula from above\n",
    "\n",
    "# You will need to adjust the source filename below to match the name of your Lab 07 data file:\n",
    "#de1 = data_entry2.sheet_copy('../Lab07/lab07_round2','lab07_data_copy')\n",
    "\n",
    "# Now calculate Z, m, and dm\n",
    "\n",
    "# Calculate chi-squared with calculated m. \n",
    "\n",
    "# It's a good idea to produce the relevant plots (scatter plot with model, and residuals plot) as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08afeee-7cb1-4807-8668-e30d7abb0d85",
   "metadata": {},
   "source": [
    "# Appendix - The full derivation of the analytic formula for minimizing chi-squared for the one-parameter model, $y=mx$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bde226e-3b00-451d-ae48-be71f10f04bc",
   "metadata": {},
   "source": [
    "From your calculus courses, you know that you can take a continuous function $f(x)$ and find the minimum or maximum (a critical point, $x_c$) by taking the derivative of $f(x)$ with respect to $x$, setting this derivative equal to zero, and solving the resulting equation\n",
    "\n",
    "$$\\left[\\frac{df(x)}{dx}\\right]_{x=x_c}=0.$$\n",
    "\n",
    "Since $\\chi^2$ is a continuous function, we can do exactly this process to come up with an expression that automatically calculates the critical point(s), to find the minimum. In other words, we can use calculus to derive an *analytic* expression for the best fit parameter(s).\n",
    "\n",
    "*NOTE! for those thinking: wait a minute, how do we know the critical point will be a minimum and not a maximum? If we think about fitting a model to data that is finite, we know that as we move the parameters to $+\\infty$ and $-\\infty$ the fit will become increasingly worse to the data, meaning that chi-squared has no maxima. From this observation is follows that any critical point found must be a minimum.*\n",
    "\n",
    "In the simplest case of a one-parameter linear model, $y=mx$, we wish to minimize chi-squared with respect to $m$ to find the best fit slope\n",
    "\n",
    "$$ \\frac{d\\chi^2(m)}{dm} = 0.$$\n",
    "\n",
    "We can first substitute our general expression for chi-squared\n",
    "\n",
    "$$ \\frac{d}{dm}\\left[ \\frac{1}{N-P} \\sum_{i=1}^N \\left(\\frac{y_i - f(x_i)}{\\delta y_i}\\right)^2 \\right] = 0.$$\n",
    "\n",
    "Our model is $f(x_i) = mx_i$, which we can substitute into the above expression\n",
    "\n",
    "$$ \\frac{d}{dm}\\left[ \\frac{1}{N-P} \\sum_{i=1}^N \\left(\\frac{y_i - mx_i}{\\delta y_i}\\right)^2 \\right] = 0.$$\n",
    "\n",
    "Since the derivative is with respect to $m$, it has no effect on $N$ or $P$, meaning we can move that leading fraction outside of the derivative.\n",
    "\n",
    "$$ \\frac{1}{N-P} \\frac{d}{dm} \\sum_{i=1}^N \\left(\\frac{y_i - mx_i}{\\delta y_i}\\right)^2 = 0.$$\n",
    "\n",
    "The summation is only over variables with a subscript \"$i$\"; $m$ does not contain this so we can also switch the order of differentiation and summation\n",
    "\n",
    "$$ \\frac{1}{N-P} \\sum_{i=1}^N \\frac{d}{dm} \\left(\\frac{y_i - mx_i}{\\delta y_i}\\right)^2 = 0.$$\n",
    "\n",
    "Now we perform some calculus and take the derivative (invoking the chain rule)\n",
    "\n",
    "$$ \\frac{2}{N-P} \\sum_{i=1}^N  \\left(\\frac{y_i - mx_i}{\\delta y_i}\\right) \\cdot \\frac{d}{dm} \\left(\\frac{y_i - mx_i}{\\delta y_i}\\right)= 0,$$\n",
    "\n",
    "$$ \\frac{2}{N-P} \\sum_{i=1}^N  \\left(\\frac{y_i - mx_i}{\\delta y_i}\\right) \\cdot \\left(-\\frac{x_i}{\\delta y_i}\\right) = 0.$$\n",
    "\n",
    "The negative sign can be taken outside the sum, and since we are setting everything equal to zero the $2/(N-P)$ can be discarded\n",
    "\n",
    "$$ \\sum_{i=1}^N  \\left(\\frac{y_i - mx_i}{\\delta y_i}\\right) \\cdot \\frac{x_i}{\\delta y_i} = 0.$$\n",
    "\n",
    "What remains is to rearrange this expression for $m$. We can start by expanding the terms in the summation\n",
    "\n",
    "$$ \\sum_{i=1}^N  \\left(\\frac{y_i}{\\delta y_i} - m\\frac{x_i}{\\delta y_i}\\right) \\cdot \\frac{x_i}{\\delta y_i} = 0$$\n",
    "$$ \\sum_{i=1}^N  \\frac{x_iy_i}{(\\delta y_i)^2} - m\\frac{x_i^2}{(\\delta y_i)^2} = 0$$\n",
    "\n",
    "then finally isolate $m$\n",
    "\n",
    "$$ m = \\frac{\\displaystyle \\sum_{i=1}^N  \\frac{x_iy_i}{(\\delta y_i)^2}}{\\displaystyle \\sum_{i=1}^N \\frac{x_i^2}{(\\delta y_i)^2}} $$\n",
    "\n",
    "So given our $x$ and $y$ data plus the uncertainty in $y$, we are able to analytically solve for the best fit slope using this equation! The uncertainty in this slope can be determined from the uncertainties in the data by uncertainty propagation. The result is:\n",
    "\n",
    "$$ \\delta m = \\sqrt{\\frac{1}{\\displaystyle \\sum_{i=1}^N \\frac{x_i^2}{(\\delta y_i)^2}}} .$$\n",
    "\n",
    "Since the term $\\sum_{i=1}^N \\frac{x_i^2}{(\\delta y_i)^2}$ appears both in $m$ and $\\delta m$, it is convenient for notation and coding purposes to define a placeholder variable\n",
    "\n",
    "$$ Z = \\sum_{i=1}^N \\frac{x_i^2}{(\\delta y_i)^2} $$\n",
    "\n",
    "such that\n",
    "\n",
    "$$ m = \\frac{1}{Z} \\sum_{i=1}^N  \\frac{x_iy_i}{(\\delta y_i)^2} $$\n",
    "$$ \\delta m = \\sqrt{\\frac{1}{Z}} .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193db82c-8f80-4c7b-8c06-3323a4d98aa0",
   "metadata": {},
   "source": [
    "# Submit\n",
    "\n",
    "Steps for submission:\n",
    "\n",
    "1. Click: Run => Run_All_Cells\n",
    "2. Read through the notebook to ensure all the cells executed correctly and without error.\n",
    "3. File => Save_and_Export_Notebook_As->HTML\n",
    "4. Inspect your exported html file.\n",
    "5. Upload the HTML document to the lab submission assignment on Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559a1799-2a4f-40e3-beaf-234900b0aa2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
